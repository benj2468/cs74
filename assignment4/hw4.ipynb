{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) (5 points) Divide the data set into test/train sets (20/80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autograd.numpy as np\n",
    "from autograd import grad \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in dataset\n",
    "import csv\n",
    "fname = \"hw4_naive.csv\"\n",
    "with open(fname) as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "header = data[0]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = [list(map(float, x[0:6])) for x in data]\n",
    "ys = [int(x[6]) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "x_train, x_test, y_train, y_test = train_test_split(xs, ys, train_size=.8, test_size=.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Implement a multinomial naive bayes classifier **from scratch** with smoothing. (set default smoothing value to 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "class MultinomialNaiveBayes():\n",
    "    def __init__(self, xs, ys, classes, smoothing = 1):\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        self.classes = classes\n",
    "        self.class_counts = [0] * len(self.classes)\n",
    "        self.smoothing = smoothing\n",
    "        for y in ys:\n",
    "            self.class_counts[self.classes.index(y)] += 1\n",
    "    \n",
    "\n",
    "    def predict(self, x):\n",
    "        counts = [deepcopy([self.smoothing] * len(x)) for i in range(0, len(self.classes))]\n",
    "        \n",
    "        for x_vals, y in zip(self.xs, self.ys):\n",
    "            i = self.classes.index(y)\n",
    "            for j, x_v in enumerate(x_vals):\n",
    "                if x_v == x[j]:\n",
    "                    counts[i][j] += 1.0        \n",
    "        joint = []\n",
    "\n",
    "        for c in range(0, len(counts)):\n",
    "            tot = 1\n",
    "            for i in range(0, len(counts[c])):\n",
    "                counts[c][i] /= (self.class_counts[c] + len(self.classes))\n",
    "                tot *= counts[c][i]\n",
    "            joint.append(tot)\n",
    "\n",
    "        return self.classes[np.argmax(joint)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Implement a Gaussian naive bayes classifier **from scratch** no smoothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from scipy.stats import norm\n",
    "\n",
    "class GaussianNaiveBayes():\n",
    "    def __init__(self, xs, ys, classes):\n",
    "        self.sds = []\n",
    "        self.means = []\n",
    "        self.classes = classes\n",
    "        filtered = [deepcopy([]) for i in range(0, len(classes))]\n",
    "        for x, y in zip(xs, ys):\n",
    "            i = self.classes.index(y)\n",
    "            filtered[i].append(x)\n",
    "\n",
    "        for i in range(0, len(classes)):\n",
    "            sds = np.std(filtered[i], axis=0)\n",
    "            means = np.mean(filtered[i], axis=0)\n",
    "            self.sds.append(sds)\n",
    "            self.means.append(means)\n",
    "\n",
    "    def predict(self, x):\n",
    "        joint = []\n",
    "\n",
    "        for c in range(0, len(self.classes)):\n",
    "            joint.append(1)\n",
    "            for i in range(0, len(x)):\n",
    "                sd = self.sds[c][i]\n",
    "                mean = self.means[c][i]\n",
    "                joint[c] *= norm.pdf(x[i], loc=mean, scale=sd)\n",
    "\n",
    "        return self.classes[np.argmax(joint)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Calculate the accuracy and the f1 score of test data using both of your models implemented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_actual,y_pred):\n",
    "    false_positive, false_negative, true_positive, true_negative = 0,0,0,0\n",
    "\n",
    "    for y_a, y_p in zip(y_actual, y_pred):\n",
    "        if y_a == y_p:\n",
    "            if y_a == 1:\n",
    "                true_positive+=1\n",
    "            else:\n",
    "                true_negative+=1\n",
    "        else:\n",
    "            if y_a == 1:\n",
    "                false_negative+=1\n",
    "            else:\n",
    "                false_positive+=1\n",
    "\n",
    "    return [false_positive, false_negative, true_positive, true_negative]\n",
    "\n",
    "def acc(matrix):\n",
    "    [_, _, tp, tn] = matrix\n",
    "    return (tp + tn) / sum(matrix)\n",
    "\n",
    "def f1(matrix):\n",
    "    [fp, fn, tp, tn] = matrix\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "\n",
    "    return 2 * (prec * rec) / (prec + rec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy under NaiveBayes: 0.8955357142857143\n",
      "F1 Score under NaiveBayes: 0.8764519535374867\n"
     ]
    }
   ],
   "source": [
    "multi = MultinomialNaiveBayes(list(x_train), list(y_train), classes=[0,1])\n",
    "y_pred = [multi.predict(x) for x in list(x_test)]\n",
    "\n",
    "matrix = evaluate(y_test, y_pred)\n",
    "print(f'Accuracy under NaiveBayes: {acc(matrix)}')\n",
    "print(f'F1 Score under NaiveBayes: {f1(matrix)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy under GaussBayes: 0.5973214285714286\n",
      "F1 Score under GaussBayes: 0.31770045385779117\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB  \n",
    "\n",
    "gauss = GaussianNaiveBayes(list(x_train), list(y_train), classes=[0,1])\n",
    "y_pred = [gauss.predict(x) for x in list(x_test)]\n",
    "\n",
    "matrix = evaluate(list(y_test), y_pred)\n",
    "print(f'Accuracy under GaussBayes: {acc(matrix)}')\n",
    "print(f'F1 Score under GaussBayes: {f1(matrix)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is to compare my code to sklearns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy under GaussBayes: 0.5955357142857143\n"
     ]
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "sc = StandardScaler()  \n",
    "x_train = sc.fit_transform(x_train)  \n",
    "x_test = sc.transform(x_test) \n",
    "classifier.fit(x_train, y_train)\n",
    "y_pred = classifier.predict(x_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(f'Accuracy under GaussBayes: {accuracy_score(y_test, y_pred) }')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "### K-means\n",
    "\n",
    "Implement a generalized K-means/median algorithm. You should have a single function that takes in as input the data points, K, and some other hyperparameters, specified below. The function should return K sets of data points. Each set corresponding to one cluster.\n",
    "The hyperparameters your functions should support and the values they can take are:\n",
    "\n",
    "- The method for calculating the centroid: Means or Median\n",
    "- The initialization method: Random Split Initialization or Random Seed Selection Method\n",
    "- Max_iter: max number of iterations to run the algorithm. \n",
    "- K: number of clusters\n",
    "\n",
    "\n",
    "Note that your stopping condition should have two parts: \n",
    "1. stop if you reach the max iterations\n",
    "2. stop if no change is made to the clusters in the last step.\n",
    "\n",
    "You will be running this code in question 3 of the assignment. For this part you just need to implement the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[12, 16, 11, 13, 14], [1, 2, 4, 3, 5]]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def _calculate_centroids(buckets, centroid_meth):\n",
    "    return [centroid_meth(list(bucket)) for bucket in buckets]\n",
    "\n",
    "def k_means(data, centroid_meth, init_meth, max_iter, K):\n",
    "    buckets = init_meth(data, K)\n",
    "\n",
    "    changes = True\n",
    "    i = 0\n",
    "    while changes and i < max_iter:\n",
    "        changes = False\n",
    "        _centers = _calculate_centroids(buckets, centroid_meth)\n",
    "        for j, bucket in enumerate(deepcopy(buckets)):\n",
    "            for x in bucket:\n",
    "                new_bucket = np.argmin([np.linalg.norm(_centers[k] - x) for k in range(0, len(_centers))])\n",
    "                if new_bucket != j:\n",
    "                    buckets[j].remove(x)\n",
    "                    buckets[new_bucket].append(x)\n",
    "                    changes = True\n",
    "        i+=1\n",
    "    return buckets\n",
    "\n",
    "def random_split(data, K):\n",
    "    groups = [deepcopy([]) for i in range(0, K)]\n",
    "    for x in data:\n",
    "        rand = int(np.random.random() * K)\n",
    "        groups[rand].append(x)\n",
    "    return groups\n",
    "\n",
    "k_means({ 1,2,3,4,5,11,12,13,14,16}, np.mean, random_split, 2, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. In this part of the assignment, you are implementing a function that calculates the SSE for a list of clusters. The function should take in a list of clusters (such as the output of the last function you implemented) and return a single SSE score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.2\n"
     ]
    }
   ],
   "source": [
    "def calculate_sse(buckets, centroid_meth):\n",
    "    _centers = _calculate_centroids(buckets, centroid_meth)\n",
    "    score = 0\n",
    "    for j, bucket in enumerate(buckets):\n",
    "        score += sum([np.linalg.norm(_centers[j] - x) for x in bucket])\n",
    "    return score\n",
    "\n",
    "buckets = k_means({ 1,2,3,4,5,11,12,13,14,16}, np.mean, random_split, 2, 2)\n",
    "print(calculate_sse(buckets, np.mean))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Run the code you implemented in question 1 for k=2,3,4,5. Set the other hyperparameters to the following:\n",
    "\n",
    "- The method for calculating the centroid: Mean\n",
    "- The initialization method: Random Split Initialization \n",
    "- Max_iterations: 100\n",
    "\n",
    "Calculate the SSE for each K using the function in question 2 and use these scores to pick the best K. What is the best K?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load in dataset\n",
    "import csv\n",
    "fname = \"hw4_cluster.csv\"\n",
    "with open(fname) as f:\n",
    "    reader = csv.reader(f, delimiter=\",\")\n",
    "\n",
    "    data = []\n",
    "    for row in reader:\n",
    "        data.append(row)\n",
    "\n",
    "header = data[0]\n",
    "data = data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs_cluster = [list(map(float, x)) for x in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum k = 5\n"
     ]
    }
   ],
   "source": [
    "best = None\n",
    "ks = [2,3,4,5]\n",
    "\n",
    "def calc_all(k):\n",
    "    buckets = k_means(xs_cluster, np.mean, random_split, 100, k)\n",
    "    return calculate_sse(buckets, np.mean)\n",
    "\n",
    "m = np.argmin(list(map(calc_all, ks)))\n",
    "\n",
    "print(f'Minimum k = {ks[m]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also did kmeans using scipy to test mine!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum k = 5\n"
     ]
    }
   ],
   "source": [
    "from scipy.cluster import vq\n",
    "\n",
    "best = None\n",
    "ks = [2,3,4,5]\n",
    "\n",
    "def calc_all(k):\n",
    "    _, err = vq.kmeans(xs_cluster, k, 100)\n",
    "    return err\n",
    "\n",
    "m = np.argmin(list(map(calc_all, ks)))\n",
    "\n",
    "print(f'Minimum k = {ks[m]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've decided to also implement HAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| - | (67.75266079355055)  \n",
      "| | - | (29.804434959936284)  \n",
      "| | | > Data: [[84.90115724  2.13144185]]\n",
      "| | | - | (26.787389070489745)  \n",
      "| | | | > Data: [[50.59583955  7.39215512]]\n",
      "| | | | > Data: [[68.1464232  27.62928008]]\n",
      "| | - | (50.08360645674771)  \n",
      "| | | - | (47.98589806274182)  \n",
      "| | | | > Data: [[ 6.20618337 99.47471097]]\n",
      "| | | | - | (41.06032018953776)  \n",
      "| | | | | - | (25.322697091900487)  \n",
      "| | | | | | - | (20.825682934389885)  \n",
      "| | | | | | | - | (3.7611242792547435)  \n",
      "| | | | | | | | > Data: [[37.9302068  63.15456551]]\n",
      "| | | | | | | | > Data: [[38.10259131 59.39739378]]\n",
      "| | | | | | | > Data: [[25.91185815 78.22263592]]\n",
      "| | | | | | > Data: [[58.95882372 62.75654532]]\n",
      "| | | | | > Data: [[ 9.5551327 38.5834121]]\n",
      "| | | > Data: [[67.60532665 99.36749482]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import itertools\n",
    "\n",
    "\n",
    "class Cluster(object):\n",
    "    def __init__(self, data = None, clusters = []):\n",
    "        self._data = data\n",
    "        self._clusters = clusters\n",
    "        self._size = 1 if np.all(data) else sum(map(lambda x: x._size, self._clusters))\n",
    "\n",
    "        self._centroid = data if np.all(data) else sum(map(lambda x: x._centroid * x._size, self._clusters)) / self._size\n",
    "\n",
    "    def join(a: Cluster, b: Cluster, dist_met):\n",
    "        new =  Cluster(clusters = [a, b])\n",
    "        new._dist = dist_met(a, b)\n",
    "        return new\n",
    "\n",
    "    def __str__(self):\n",
    "        if np.all(self._data):\n",
    "            return f'Data: [{self._data}]'\n",
    "        else:\n",
    "            return f'Size: {self._size} Center: {self._centroid}'\n",
    "\n",
    "    def euclid(a: Cluster, b: Cluster):\n",
    "        return np.linalg.norm(a._centroid - b._centroid)\n",
    "    \n",
    "    def print(cluster: Cluster, prefix: str):\n",
    "        \n",
    "        if (np.all(cluster._data)):\n",
    "            return prefix + '| > ' + str(cluster) + '\\n'\n",
    "        res = prefix + f'| - | ({cluster._dist})  \\n' \n",
    "        for c in cluster._clusters:\n",
    "            sub = Cluster.print(c, prefix + '| ')\n",
    "            res += sub\n",
    "        return res\n",
    "\n",
    "def hac_clustering(data, dist_met):\n",
    "    clusters = set(map(lambda x: Cluster(data = x), data))\n",
    "    while (len(clusters) > 1):\n",
    "        combos = list(itertools.combinations(clusters, 2))\n",
    "        (c1, c2) = combos[np.argmin(list(map(lambda x: dist_met(*x), combos)))]\n",
    "        c3 = Cluster.join(c1, c2, dist_met)\n",
    "        clusters.remove(c1)\n",
    "        clusters.remove(c2)\n",
    "        clusters.add(c3)\n",
    "    return clusters\n",
    "\n",
    "cluster = hac_clustering(np.multiply(np.random.rand(10, 2), 100), Cluster.euclid).pop()\n",
    "print(Cluster.print(cluster, ''))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
  },
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
